# -*- coding: utf-8 -*-
"""kelly_replication.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11D5Cr_RXdWjOP6CTZcVf16jmsPiThhHU
"""

# All imports for future code blocks
!pip install -q "rapids-colab==23.10"
import os, gc, json, re, functools, textwrap
import numpy as np, pandas as pd, matplotlib.pyplot as plt
import pyarrow as pa, pyarrow.parquet as pq, pyarrow.dataset as ds
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from tqdm.auto import tqdm

import cudf, cupy as cp
from cuml.preprocessing import OneHotEncoder
from cuml.ensemble import RandomForestRegressor as cuRF

# Google colab drive set up
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
DATA_DIR = '/content/drive/MyDrive/capstone'


# First we build the macro data:
# the 8 predictors from Welch-Goyal (“We construct eight monthly macroeconomic
#  predictors … and lag each predictor by one month.”)

# 1. Risk‑free (3‑mo T‑bill) from fred
fred = (pd.read_csv(f'{DATA_DIR}/fred.csv', parse_dates=['observation_date'])
          .rename(columns={'observation_date':'month', 'TB3MS':'tbl'})
          .assign(month=lambda d: d['month'].dt.to_period('M')
                                             .dt.to_timestamp('M'),
                  tbl=lambda d: d['tbl']/100)
          .loc[:, ['month','tbl']])

# 2. Welch–Goyal original 2005 file  →  tbl lty ltr tms dfy dfr infl
wg = (pd.read_csv(f'{DATA_DIR}/goyal-welch_original.csv',
                  na_values=['NaN',''],
                  converters={'Index': lambda s: s.replace(',','')})
        .rename(columns={'yyyymm':'month'})
        .loc[:, ['month','tbl','lty','ltr','AAA','BAA','infl','corpr']]
        .assign(month=lambda d: pd.to_datetime(d['month'], format='%Y%m')
                                   .dt.to_period('M').dt.to_timestamp('M'),
                tbl = lambda d: d['tbl']/100,
                lty = lambda d: d['lty']/100,
                dfy = lambda d: (d['BAA']-d['AAA'])/100,
                tms = lambda d: d['lty']-d['tbl'],
                dfr = lambda d: d['corpr']-d['ltr'])
        .loc[:, ['month','tbl','lty','ltr','tms','dfy','dfr','infl']]
        .query('month >= "1957-01-01"')
        .assign(**{c: lambda d, col=c: d[col].shift(1)
                   for c in ['tbl','lty','ltr','tms','dfy','dfr','infl']}))

# 3. Investment‑to‑capital ratio  ik  from fred's PNFI & K1NTOTL1ES000

PNFI_CSV = f'{DATA_DIR}/PNFI.csv'              # quarterly
KN_CSV   = f'{DATA_DIR}/K1NTOTL1ES000.csv'     # annual

pnfi = (pd.read_csv(PNFI_CSV, parse_dates=['DATE'])
          .rename(columns={'DATE':'month', 'VALUE':'pnfi'})
          .assign(month=lambda d: d['month'].dt.to_period('M')
                                         .dt.to_timestamp('M'))
          .set_index('month')
          .resample('ME').ffill())             # Q → month-end

kn = (pd.read_csv(KN_CSV, parse_dates=['observation_date'])
         .rename(columns={'observation_date':'month',
                          'K1NTOTL1ES000':'kn'})
         .assign(month=lambda d: d['month'].dt.to_period('M')
                                        .dt.to_timestamp('M'))
         .set_index('month')
         .resample('ME').ffill())             # A → month-end

ik = (pd.concat([pnfi, kn], axis=1)
        .assign(ik=lambda d: d['pnfi']/d['kn'])
        .loc[:, ['ik']]
        .assign(ik=lambda d: d['ik'].shift(1))
        .reset_index())                       # back to column

# 4. Merge Welch, ik, and keep FRED tbl for early months not covered by WG

macro = (fred.merge(wg, on='month', how='left', suffixes=('_fred',''))
              .assign(tbl=lambda d: d['tbl'].fillna(d['tbl_fred']))
              .drop(columns=['tbl_fred'])
              .merge(ik, on='month', how='left')
              .loc[:, ['month','tbl','lty','ltr','tms',
                       'dfy','dfr','infl','ik']]
              .add_prefix('macro_'))

# cast numerics only (exclude macro_month which is datetime)
num_cols = macro.columns.difference(['macro_month'])
macro[num_cols] = macro[num_cols].astype('float32')

# 5. sanity checks
print('macro shape:', macro.shape)        # ≈ 820 × 9
dup = macro['macro_month'].duplicated().any()
na_share = macro[num_cols].isna().mean().round(4)

print('duplicates in month index?', dup)
# There should be ~28.3% NA share because WG starts in 1973, whereas Kelly is 1957
# We will set the NA values to 0 as per Kelly
#  “Any missing predictor values—either characteristics or macro variables—
#   are set to zero after the scaling step.”
print('NA share per column:\n', na_share)

assert not dup, 'Duplicate months detected in macro series'

# Now we process the kelly csv
KELLY_CSV = os.path.join(DATA_DIR, 'kelly.csv')
kelly     = pd.read_csv(KELLY_CSV, parse_dates=['DATE'], low_memory=False)

# We clean and lag the characteristics

kelly.rename(columns={'PERMNO': 'permno', 'DATE': 'month'}, inplace=True)
kelly['month'] = (kelly['month']
                    .dt.to_period('M')
                    .dt.to_timestamp('M'))          # month-end stamp
kelly.sort_values(['permno', 'month'], inplace=True)

# ID columns
id_cols = {'permno', 'month', 'sic2'}
kelly['sic2']   = (kelly['sic2']
                    .astype(object)
                    .fillna('UNKNOWN')
                    .astype('category'))
kelly['permno'] = kelly['permno'].astype('int32')

# Determine the lag map (annual vs quartlery vs monthly)
char_cols = [c for c in kelly.columns if c not in id_cols]

g = (kelly.sort_values(['permno', 'month'])
          .groupby('permno')[char_cols])
share_changed = g.apply(lambda df: df.ffill()
                                   .nunique() / len(df)).mean()

lag_map = {**{c: 6 for c in share_changed[share_changed <= 0.35].index},      # annual
           **{c: 4 for c in share_changed[(share_changed > 0.35) &
                                          (share_changed <= 0.80)].index},    # quarterly
           **{c: 1 for c in share_changed[share_changed > 0.80].index}}       # monthly

for col, lag in lag_map.items():
    kelly[col] = kelly.groupby('permno')[col].shift(lag)

# Add prefix
kelly.rename(columns={c: f'char_{c}' for c in char_cols}, inplace=True)
predictor_cols = [c for c in kelly.columns if c.startswith('char_')]

# Fill missing: cross-section median; then 0
# "Missing characteristic observations are first replaced by the cross-sectional
#  median in that month. Any remaining missing values are set to zero "
med = kelly.groupby('month')[predictor_cols].transform('median')
kelly[predictor_cols] = kelly[predictor_cols].fillna(med).fillna(0.0)

# Checks
na_flag = kelly[predictor_cols].isna().any().any()
cf = lambda c: (kelly[c] == kelly.groupby('permno')[c].shift(1)).mean()
corrs = {c: round(cf(c), 3) for c in predictor_cols if c[5:] in lag_map}

print('NA left?', na_flag)                       # must be False
print('lag-logic check (mean equality):', corrs) # annual≈0.92, qtr≈0.67

# 4 CRSP processing: lagged mkt-cap, total & excess returns
#     (now using the full WRDS extract crsp_2.csv that *does* include DLRET)

CRSP_CSV = os.path.join(DATA_DIR, 'crsp_2.csv')
crsp     = pd.read_csv(CRSP_CSV, parse_dates=['date'], low_memory=False)

need = ['PERMNO', 'date', 'PRC', 'SHROUT', 'RET', 'DLRET']   # DLRET present
crsp_sub = (crsp[need]
              .rename(columns={'PERMNO':'permno', 'date':'month'})
              .assign(
                  month  = lambda d: pd.to_datetime(d['month'])
                                       .dt.to_period('M')
                                       .dt.to_timestamp('M'),
                  PRC    = lambda d: pd.to_numeric(d['PRC'],
                                                   errors='coerce').abs(),
                  SHROUT = lambda d: pd.to_numeric(d['SHROUT'],
                                                   errors='coerce')
                                       .mul(1_000),          # ’000 → shares
                  RET    = lambda d: pd.to_numeric(d['RET'],
                                                   errors='coerce').fillna(0),
                  DLRET  = lambda d: pd.to_numeric(d['DLRET'],
                                                   errors='coerce').fillna(0))
              .assign(
                  mktcap = lambda d: d['PRC'] * d['SHROUT'],
                  totret = lambda d: (1 + d['RET']) * (1 + d['DLRET']) - 1)
              .sort_values(['permno', 'month'])
              .astype({'PRC':'float32', 'SHROUT':'float32',
                       'mktcap':'float32', 'totret':'float32'}))

# risk-free monthly return (already decimal) from the tbl series we built earlier
rf = fred.rename(columns={'tbl':'rf_month'})[['month','rf_month']]

crsp_sub = (crsp_sub.merge(rf, on='month', how='left')
                       .assign(exret=lambda d: d['totret'] - d['rf_month']))

# one-month lag of market cap
crsp_sub['mktcap_lag'] = (crsp_sub.groupby('permno', sort=False)['mktcap']
                                       .shift(1))

print('CRSP:', crsp_sub.shape,
      '| missing exret:', crsp_sub['exret'].isna().mean(),
      '| missing mktcap_lag:', crsp_sub['mktcap_lag'].isna().mean())

# 5 merge

macro_ren = macro.rename(columns={'macro_month': 'month'})
full = (kelly
        .merge(crsp_sub[['permno', 'month', 'exret', 'mktcap_lag']],
               on=['permno', 'month'], how='inner')
        .merge(macro_ren, on='month', how='left')
        .rename(columns={'exret': 'ret_excess'})
        .sort_values(['month', 'permno'])
        .reset_index(drop=True))

# ffill first-month gap for each firm’s lagged size
full['mktcap_lag'] = (full.sort_values(['permno', 'month'])
                           .groupby('permno')['mktcap_lag']
                           .ffill())

# Kelly rule: 0-impute *numeric* non-predictor cols; leave categoricals alone
predictor_set   = set(predictor_cols)
id_set          = {'permno', 'month'}
non_pred        = [c for c in full.columns if c not in predictor_set.union(id_set)]

num_cols  = full[non_pred].select_dtypes(include='number').columns
full[num_cols] = full[num_cols].fillna(0.0)

# final sanity
assert full.drop(columns=['permno', 'month']).isna().sum().sum() == 0
print(f'✅  Final merged DF → {full.shape[0]:,} rows × {full.shape[1]} columns')
print(full.head(3))

# =====================================================================
# 6️⃣  DISK-BACKED PIPELINE  (CPU, exact Kelly-Xiu spec)
#     • writes Parquet once, then streams 200 k-row batches
#     • keeps full char × macro interactions
#     • trains scikit RF incrementally (warm_start)
# =====================================================================

import os, gc, math, functools, json, textwrap
import numpy as np, pandas as pd, pyarrow as pa, pyarrow.parquet as pq
import pyarrow.dataset as ds
from tqdm.auto import tqdm
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

WORKDIR = "/content/tmp_tidy"
os.makedirs(WORKDIR, exist_ok=True)

# ---------- 1. write base.parquet  (id + y + chars + macro + SIC) ----------
sic_dummies = pd.get_dummies(full["sic2"], prefix="industry", dtype="float32")

base_cols = (
    ["permno", "month", "ret_excess", "mktcap_lag"] +
    predictor_cols +
    [c for c in full.columns if c.startswith("macro_")]
)
base_df = pd.concat([full[base_cols].reset_index(drop=True),
                     sic_dummies.reset_index(drop=True)], axis=1)

def _month_int(s): return s.dt.year * 100 + s.dt.month

if not os.path.exists(f"{WORKDIR}/base.parquet"):
    tbl = pa.Table.from_pandas(base_df, preserve_index=False)           \
            .append_column("month_int",
                          pa.array(_month_int(full["month"]), pa.int32())) \
            .append_column("year",
                          pa.array(full["month"].dt.year, pa.int16()))
    pq.write_table(tbl, f"{WORKDIR}/base.parquet", compression="zstd")
    del tbl; gc.collect()
    print("✅  base.parquet written")
else:
    print("↩︎  base.parquet already exists – skipped")

## ---------- 2. write interaction shards  (char × single macro) ------------
CHAR_BLOCK = full[predictor_cols].to_numpy("float32", copy=False)
macro_vars = [c for c in full.columns if c.startswith("macro_")]

def write_interaction_shard(macro_name: str, chunk_rows: int = 500_000):
    out_path = f"{WORKDIR}/interact_{macro_name}.parquet"
    if os.path.exists(out_path):
        return                                         # already on disk

    macro_vec  = full[macro_name].to_numpy("float32", copy=False)
    col_names  = [f"{c}:{macro_name}" for c in predictor_cols]
    writer = None

    for r0 in range(0, len(full), chunk_rows):
        r1    = min(r0 + chunk_rows, len(full))
        block = CHAR_BLOCK[r0:r1] * macro_vec[r0:r1, None]

        tbl   = pa.Table.from_arrays(
                    [pa.array(block[:, j]) for j in range(block.shape[1])],
                    names=col_names)

        if writer is None:
            writer = pq.ParquetWriter(out_path, tbl.schema, compression="zstd")
        writer.write_table(tbl)

        del block, tbl; gc.collect()

    writer.close()
    print("  shard", macro_name, "written")

#  --- actually write shards (was missing) -----------------------------------
for m in tqdm(macro_vars, desc="writing interaction shards"):
    write_interaction_shard(m)

# ---------- 3. streaming batch generator  -----------------------------------
BASE_DS   = ds.dataset(f"{WORKDIR}/base.parquet", format="parquet")
CHAR_COLS = [c for c in BASE_DS.schema.names if c.startswith("char_")]
MACRO_COLS= [c for c in BASE_DS.schema.names if c.startswith("macro_")]
ID_LIKE   = {"permno", "month", "month_int", "year"}    # true IDs

@functools.lru_cache(maxsize=8)
def _inter_ds(macro):
    return ds.dataset(f"{WORKDIR}/interact_{macro}.parquet", format="parquet")

# ── helpers (add right above the OOS loop) ──────────────────────────────────
def iter_batches(year_slice=None, batch_rows=200_000):
    """Yield (X, y) NumPy batches for an inclusive year_slice=(lo, hi)."""
    filt = None
    if year_slice:
        lo, hi = year_slice
        filt = (ds.field("year") >= lo) & (ds.field("year") <= hi)

    row_ptr = 0
    for rb in BASE_DS.to_batches(batch_size=batch_rows, filter=filt):
        pdf  = rb.to_pandas()
        y    = pdf.pop("ret_excess").to_numpy("float32", copy=False)
        size = pdf.pop("mktcap_lag").to_numpy("float32", copy=False).reshape(-1,1)
        pdf.drop(columns=ID_LIKE, inplace=True)

        X_parts = [pdf.astype("float32", copy=False), size]
        for m in MACRO_COLS:
            cols = [f"{c}:{m}" for c in CHAR_COLS]
            tbl  = (_inter_ds(m)
                    .to_table(columns=cols)
                    .slice(row_ptr, rb.num_rows))
            X_parts.append(tbl.to_pandas().astype("float32", copy=False))

        arrays = [(a if isinstance(a, np.ndarray) else
                   a.to_numpy("float32", copy=False)) for a in X_parts]
        X = np.hstack(arrays)
        yield X, y
        row_ptr += rb.num_rows
        del pdf, X_parts, arrays, X, y; gc.collect()

def collect_batches(year_lo, year_hi):
    xs, ys = [], []
    for Xb, yb in iter_batches(year_slice=(year_lo, year_hi)):
        xs.append(Xb); ys.append(yb)
    if not xs: return None, None
    return np.vstack(xs), np.concatenate(ys)
# ---------- 4. incremental RF training (300 trees total) --------------------
rf = RandomForestRegressor(n_estimators=0, warm_start=True,
                           max_depth=4, max_features='sqrt',
                           random_state=0, n_jobs=-1)
TREES_PER_PASS = 50

for pass_no, (Xb, yb) in enumerate(iter_batches(), start=1):
    rf.n_estimators += TREES_PER_PASS
    rf.fit(Xb, yb)
    print(f'pass {pass_no}: trees = {rf.n_estimators}')
    gc.collect()

# ---------- 5. quick in-sample RMSE on the last batch -----------------------
from sklearn.metrics import mean_squared_error
import numpy as np

mse  = mean_squared_error(yb, rf.predict(Xb))
rmse = np.sqrt(mse)
print(f'final batch RMSE = {rmse:.5f}')

# 🔄  RESET host + GPU memory   (run once before GPU OOS loop)
import gc, cupy as cp

for var in ["full", "CHAR_BLOCK", "sic_dummies", "rf", "Xb", "yb"]:
    if var in globals():
        del globals()[var]
gc.collect()

cp.get_default_memory_pool().free_all_blocks()
cp.get_default_pinned_memory_pool().free_all_blocks()
print("✅  RAM & VRAM flushed")

# ===============================================================
# 8️⃣  GPU year-by-year OOS loop
#     • chars + macros + SIC + size   (NO char×macro interactions)
#     • adaptive row–sampling so every slice fits A100 VRAM
#     • depth tuned on 12-year rolling window
# ===============================================================
import cupy as cp, numpy as np, gc, json
from cuml.ensemble import RandomForestRegressor as cuRF
from sklearn.metrics import mean_squared_error
from tqdm.auto import tqdm

DEPTH_GRID  = [2, 3, 4, 5, 6]
N_TREES     = 300
SEED        = 0
MAX_GB      = 6.0          # target VRAM ≤ 6 GB per slice

k_char  = len(predictor_cols)
k_macro = 8
k_sic   = len([c for c in base_df.columns if c.startswith("industry_")])
COLS_GPU = k_char + k_macro + k_sic + 1          # +1 for size

def gpu_block(year_lo, year_hi):
    """
    Return (X_gpu, y_gpu) for rows in [year_lo, year_hi],
    down-sampling until estimated VRAM use < MAX_GB.
    """
    X_cpu, y_cpu = collect_batches(year_lo, year_hi)
    if X_cpu is None:
        return None, None

    need_bytes = lambda n: n * COLS_GPU * 4          # float32
    keep_frac  = min(1.0, MAX_GB * 1e9 / need_bytes(len(y_cpu)))
    rng        = np.random.RandomState(0)
    mask       = rng.rand(len(y_cpu)) < keep_frac
    Xs, ys     = X_cpu[mask], y_cpu[mask]

    X_parts = [
        cp.asarray(Xs[:, :k_char]                     , dtype=cp.float32),
        cp.asarray(Xs[:, k_char:k_char+k_macro]       , dtype=cp.float32),
        cp.asarray(Xs[:, k_char+k_macro:k_char+k_macro+k_sic],
                   dtype=cp.float32),
        cp.asarray(Xs[:, -1:]                         , dtype=cp.float32)
    ]
    return cp.hstack(X_parts), cp.asarray(ys, dtype=cp.float32)

def free_gpu():
    cp._default_memory_pool.free_all_blocks()
    cp.cuda.runtime.deviceSynchronize()

best_depth = {}
pred_all, true_all = [], []

for yr in tqdm(range(1987, 2017), desc="GPU OOS"):
    # --------- pick depth on preceding 12-year window --------------------
    val_end, val_start = yr - 1, yr - 12
    X_tr , y_tr  = gpu_block(1957 , val_start - 1)
    X_val, y_val = gpu_block(val_start, val_end)

    d_star, rmse_star = None, np.inf
    for d in DEPTH_GRID:
        rf = cuRF(n_estimators=N_TREES, max_depth=d,
                  max_features='sqrt', n_streams=1, random_state=SEED)
        rf.fit(X_tr, y_tr)
        rmse = np.sqrt(
    mean_squared_error(cp.asnumpy(y_val),
                       cp.asnumpy(rf.predict(X_val)))
)
        if rmse < rmse_star:
            d_star, rmse_star = d, rmse
        del rf; free_gpu()

    best_depth[yr] = d_star
    del X_tr, y_tr, X_val, y_val; free_gpu()

    # --------- train on train+val, predict OOS year ----------------------
    X_tv , y_tv  = gpu_block(1957, yr - 1)
    X_oos, y_oos = gpu_block(yr   , yr)

    rf = cuRF(n_estimators=N_TREES, max_depth=d_star,
              max_features='sqrt', n_streams=1, random_state=SEED)
    rf.fit(X_tv, y_tv)
    pred_all.append(cp.asnumpy(rf.predict(X_oos)))
    true_all.append(cp.asnumpy(y_oos))

    del rf, X_tv, y_tv, X_oos, y_oos; free_gpu()

# ------------- aggregate OOS performance -----------------------------------
pred  = np.concatenate(pred_all)
truth = np.concatenate(true_all)
oos_rmse = np.sqrt(mean_squared_error(truth, pred))
print(f"OOS RMSE 1987-2016 (no interactions, adapt-sample) = {oos_rmse:.5f}")
print("Depth choices by year:", json.dumps(best_depth, indent=2))

# ===============================================================
# 9  GPU final refit per OOS year → write predictions
#       • uses chars + macros + SIC dummies + size only
#       • no interactions  → fits in 35 GB VRAM
# ===============================================================
import cupy as cp, numpy as np, gc, os, pyarrow.parquet as pq
from cuml.ensemble import RandomForestRegressor as cuRF
from sklearn.metrics import mean_squared_error

OUT_DIR = f"{WORKDIR}/oos_preds"
os.makedirs(OUT_DIR, exist_ok=True)

# --- constants defined earlier ---------------------------------
k_char  = len(predictor_cols)
k_macro = 8
k_sic   = len([c for c in BASE_DS.schema.names if c.startswith("industry_")])
COLS_GPU = k_char + k_macro + k_sic + 1          # +1 for size (mktcap_lag)

# ---------- helpers --------------------------------------------
def gpu_block(year_lo, year_hi, max_gb=20):
    """Full slice → GPU (chars+macro+sic+size). Down-sample if > max_gb."""
    X_cpu, y_cpu = collect_batches(year_lo, year_hi)
    if X_cpu is None:
        return None, None
    bytes_needed = lambda n: n * COLS_GPU * 4
    frac = min(1.0, max_gb * 1e9 / bytes_needed(len(y_cpu)))
    mask = np.random.RandomState(0).rand(len(y_cpu)) < frac
    Xs, ys = X_cpu[mask], y_cpu[mask]
    X_gpu  = cp.asarray(Xs[:, :COLS_GPU], dtype=cp.float32, order="C")
    y_gpu  = cp.asarray(ys,            dtype=cp.float32)
    return X_gpu, y_gpu

def to_gpu_pred(X_cpu_batch):
    """CPU batch → CuPy, keep first COLS_GPU columns only."""
    return cp.asarray(X_cpu_batch[:, :COLS_GPU], dtype=cp.float32, order="C")

def free_gpu():
    cp.get_default_memory_pool().free_all_blocks()
    cp.cuda.runtime.deviceSynchronize()

# ---------- refit & predict ------------------------------------
for oos_year, depth in best_depth.items():

    out_path = f"{OUT_DIR}/preds_{oos_year}.parquet"
    if os.path.exists(out_path):
        print("↩︎", oos_year, "already done – skip");  continue

    # train on all data up to Dec (oos_year-1)
    X_tv, y_tv = gpu_block(1957, oos_year - 1)
    rf = cuRF(n_estimators=300, max_depth=depth,
              max_features='sqrt', n_streams=1, random_state=0)
    rf.fit(X_tv, y_tv)
    del X_tv, y_tv; free_gpu()

    preds = []
    for X_cpu, _ in iter_batches(year_slice=(oos_year, oos_year)):
        if X_cpu.size == 0: continue
        X_gpu = to_gpu_pred(X_cpu)
        preds.append(cp.asnumpy(rf.predict(X_gpu)))
        del X_gpu; free_gpu()

    # identifiers for that year
    id_tbl = BASE_DS.to_table(columns=['permno', 'month', 'year'],
                              filter=(ds.field('year') == oos_year))
    id_df  = id_tbl.to_pandas()
    id_df['rf_pred'] = np.concatenate(preds).astype('float32')

    pq.write_table(pa.Table.from_pandas(id_df), out_path, compression="zstd")
    print(f"✅  {oos_year}: {len(id_df):,} predictions saved")

    del rf, id_tbl, id_df, preds; free_gpu()

# ---------- aggregate OOS metrics -----------------------------------------
all_preds, all_true = [], []
for yr in range(1987, 2017):
    p_path = f"{OUT_DIR}/preds_{yr}.parquet"
    if not os.path.exists(p_path): continue
    df  = pq.read_table(p_path).to_pandas()
    _, y_true = collect_batches(yr, yr)
    all_preds.append(df['rf_pred'].values)
    all_true .append(y_true)

y_pred_full = np.concatenate(all_preds)
y_true_full = np.concatenate(all_true)

rmse = np.sqrt(mean_squared_error(y_true_full, y_pred_full))
from sklearn.metrics import r2_score
print(f"Final OOS RMSE 1987-2016 = {rmse:.5f}")
print(f"Final OOS R²            = {r2_score(y_true_full, y_pred_full):.5f}")

# ===============================================================
# 9️⃣  Elastic-Net on GPU  (chars + macros + SIC + size, no interactions)
#      • α tuned on rolling 12-year window (grid from GKX appendix)
#      • produces yearly prediction Parquets  →  {WORKDIR}/enet_oos_preds
# ===============================================================
import cupy as cp, numpy as np, gc, os, json, pyarrow.parquet as pq
import pyarrow.dataset as ds, pyarrow as pa
from cuml.linear_model import ElasticNet
from sklearn.metrics   import mean_squared_error
from tqdm.auto         import tqdm

OUT_DIR = f"{WORKDIR}/enet_oos_preds"
os.makedirs(OUT_DIR, exist_ok=True)

# ---------- constants -------------------------------------------------------
ALPHA_GRID = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]   # same set Gu-Kelly-Xiu use
L1_RATIO   = 0.5                              # paper default
MAX_GB     = 20                               # full slice always < 20 GB

k_char  = len(predictor_cols)
k_macro = 8
k_sic   = len([c for c in BASE_DS.schema.names if c.startswith("industry_")])
COLS_GPU = k_char + k_macro + k_sic + 1       # +1 for lagged size

# ---------- slice helpers ---------------------------------------------------
def gpu_slice(year_lo, year_hi):
    """Return full slice (chars+macro+sic+size) on GPU."""
    X_cpu, y_cpu = collect_batches(year_lo, year_hi)
    if X_cpu is None: return None, None
    X_gpu = cp.asarray(X_cpu[:, :COLS_GPU], dtype=cp.float32, order="C")
    y_gpu = cp.asarray(y_cpu,              dtype=cp.float32)
    return X_gpu, y_gpu

def free_gpu():
    cp.get_default_memory_pool().free_all_blocks()
    cp.cuda.runtime.deviceSynchronize()

# ---------- α-selection + OOS prediction loop -------------------------------
best_alpha = {}
pred_cat, true_cat = [], []

for yr in tqdm(range(1987, 2017), desc="Elastic-Net OOS"):

    # ----- tune α on preceding 12-year window ---------------------------
    val_end, val_start = yr-1, yr-12
    X_tr , y_tr  = gpu_slice(1957,  val_start-1)
    X_val, y_val = gpu_slice(val_start, val_end)

    a_star, rmse_star = None, np.inf
    for a in ALPHA_GRID:
        en = ElasticNet(alpha=a, l1_ratio=L1_RATIO)
        en.fit(X_tr, y_tr)
        rmse = np.sqrt(mean_squared_error(cp.asnumpy(y_val),
                                          cp.asnumpy(en.predict(X_val))))
        if rmse < rmse_star:
            a_star, rmse_star = a, rmse
        del en; free_gpu()
    best_alpha[yr] = a_star
    del X_tr, y_tr, X_val, y_val; free_gpu()

    # ----- refit on train+val, predict OOS year -------------------------
    X_tv , y_tv  = gpu_slice(1957, yr-1)
    X_oos, y_oos = gpu_slice(yr,    yr)

    en = ElasticNet(alpha=a_star, l1_ratio=L1_RATIO)
    en.fit(X_tv, y_tv)
    y_hat = cp.asnumpy(en.predict(X_oos))

    # ----- save predictions --------------------------------------------
    id_tbl = BASE_DS.to_table(columns=['permno', 'month', 'year'],
                              filter=(ds.field('year') == yr))
    id_df  = id_tbl.to_pandas()
    id_df.rename(columns={'month': 'month_ts'}, inplace=True)
    id_df['enet_pred'] = y_hat.astype('float32')
    pq.write_table(pa.Table.from_pandas(id_df),
                   f"{OUT_DIR}/enet_{yr}.parquet", compression="zstd")
    print(f"✅  {yr}: α={a_star}  →  {len(id_df):,} predictions saved")

    pred_cat.append(y_hat);  true_cat.append(cp.asnumpy(y_oos))
    del en, X_tv, y_tv, X_oos, y_oos, id_tbl, id_df; free_gpu()

# ---------- aggregate OOS performance --------------------------------------
y_pred = np.concatenate(pred_cat);  y_true = np.concatenate(true_cat)
oos_rmse = np.sqrt(mean_squared_error(y_true, y_pred))
from sklearn.metrics import r2_score
print(f"OOS RMSE 1987-2016 (Elastic-Net) = {oos_rmse:.5f}")
print(f"OOS R²                           = {r2_score(y_true, y_pred):.5f}")
print("Chosen α per year:", json.dumps(best_alpha, indent=2))

# ===============================================================
# 🔥  Neural Net (full-batch, AMP FP16) – yearly OOS predictions
#      • 2-layer MLP 128-64, BN, Dropout 0.2  (GKX spec)
#      • Trains on ALL rows for each window in one batch
#      • 50 epochs, AdamW lr=1e-3
#      • Saves {WORKDIR}/nn_oos_preds_full/nn_<year>.parquet
#      • Runs ≈15 min on a 40 GB A100, < 30 GB VRAM
# ===============================================================
import torch, torch.nn as nn, cupy as cp, numpy as np, os, gc, pyarrow as pa
import pyarrow.parquet as pq, pyarrow.dataset as ds
from tqdm.auto import tqdm
from sklearn.metrics import mean_squared_error

DEVICE   = 'cuda'
EPOCHS   = 50
WORK_OUT = f"{WORKDIR}/nn_oos_preds_full"
os.makedirs(WORK_OUT, exist_ok=True)

k_char  = len(predictor_cols)
k_macro = 8
k_sic   = len([c for c in BASE_DS.schema.names if c.startswith("industry_")])
COLS    = k_char + k_macro + k_sic + 1            # +1 for mktcap_lag

# ---------- model -----------------------------------------------------------
class MLP(nn.Module):
    def __init__(self, d_in):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_in, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(128, 64),  nn.BatchNorm1d(64),  nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(64, 1)
        )
    def forward(self, x): return self.net(x)

# ---------- helpers ---------------------------------------------------------
def slice_gpu(year_lo, year_hi):
    X_cpu, y_cpu = collect_batches(year_lo, year_hi)
    if X_cpu is None: return None, None
    X = torch.tensor(X_cpu[:, :COLS], dtype=torch.float32, device=DEVICE)
    y = torch.tensor(y_cpu,           dtype=torch.float32, device=DEVICE)
    return X, y

def free_gpu():
    torch.cuda.empty_cache()
    cp.get_default_memory_pool().free_all_blocks()

pred_all, true_all = [], []

for yr in tqdm(range(1987, 2017), desc="NN OOS Full-batch"):
    # ---------- 1. build training & OOS tensors -------------------------
    X_train, y_train = slice_gpu(1957, yr-1)
    X_oos  , y_oos   = slice_gpu(yr,    yr)

    # ---------- 2. initialise model & optimiser ------------------------
    model = MLP(COLS).to(DEVICE)
    opt   = torch.optim.AdamW(model.parameters(), lr=1e-3)
    lossf = nn.MSELoss()
    scaler = torch.cuda.amp.GradScaler()

    # ---------- 3. full-batch training (AMP) ---------------------------
    model.train()
    for _ in range(EPOCHS):
        opt.zero_grad(set_to_none=True)
        with torch.amp.autocast('cuda', dtype=torch.float16):
            pred = model(X_train).squeeze(1)
            loss = lossf(pred, y_train)
        scaler.scale(loss).backward()
        scaler.step(opt)
        scaler.update()

    # ---------- 4. predict OOS year -----------------------------------
    model.eval()
    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.float16):
        y_hat = model(X_oos).squeeze(1).cpu().numpy()

    # ---------- 5. save Parquet ---------------------------------------
    id_tbl = BASE_DS.to_table(columns=['permno', 'month', 'year'],
                              filter=(ds.field('year') == yr))
    id_df  = id_tbl.to_pandas()
    id_df.rename(columns={'month': 'month_ts'}, inplace=True)
    id_df['nn_pred'] = y_hat.astype('float32')
    pq.write_table(pa.Table.from_pandas(id_df),
                   f"{WORK_OUT}/nn_{yr}.parquet", compression="zstd")
    print(f"✅  {yr}: {len(id_df):,} preds saved")

    pred_all.append(y_hat);  true_all.append(y_oos.cpu().numpy())
    del model, X_train, y_train, X_oos, y_oos, id_tbl, id_df;  free_gpu()

# ---------- 6. aggregate OOS performance  -------------------------------
y_pred = np.concatenate(pred_all)
y_true = np.concatenate(true_all)

mask = ~np.isnan(y_true)                # drop firms with missing ret_excess
y_pred = y_pred[mask]
y_true = y_true[mask]

from sklearn.metrics import mean_squared_error, r2_score
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2   = r2_score(y_true, y_pred)

print(f"OOS RMSE 1987-2016 (NN full-batch) = {rmse:.5f}")
print(f"OOS R²                              = {r2:.5f}")

# ===============================================================
# ⚡  XGBoost GPU (“gpu_hist”) – yearly OOS predictions
#      • depth=3, 1000 trees, eta 0.05   (paper default)
#      • full rows, no interactions
#      • saves xgb_<year>.parquet in {WORKDIR}/xgb_oos_preds
# ===============================================================
import xgboost as xgb, numpy as np, cupy as cp, os, gc, pyarrow as pa
import pyarrow.parquet as pq, pyarrow.dataset as ds
from tqdm.auto import tqdm
from sklearn.metrics import mean_squared_error

WORK_OUT = f"{WORKDIR}/xgb_oos_preds"
os.makedirs(WORK_OUT, exist_ok=True)

k_char  = len(predictor_cols)
k_macro = 8
k_sic   = len([c for c in BASE_DS.schema.names if c.startswith("industry_")])
COLS    = k_char + k_macro + k_sic + 1        # +1 for mktcap_lag

def matrix_slice(year_lo, year_hi):
    X_cpu, y_cpu = collect_batches(year_lo, year_hi)
    if X_cpu is None: return None
    return xgb.DMatrix(X_cpu[:, :COLS], label=y_cpu)

params = dict(tree_method='gpu_hist', max_depth=3,
              eta=0.05, subsample=0.5, colsample_bytree=0.8,
              objective='reg:squarederror', gpu_id=0)

pred_all, true_all = [], []

for yr in tqdm(range(1987, 2017), desc="XGBoost OOS"):
    d_train = matrix_slice(1957, yr-1)
    d_oos   = matrix_slice(yr, yr)

    bst = xgb.train(params, d_train, num_boost_round=1000, verbose_eval=False)
    y_hat = bst.predict(d_oos)

    # save Parquet
    id_tbl = BASE_DS.to_table(columns=['permno','month','year'],
                              filter=(ds.field('year') == yr))
    id_df  = id_tbl.to_pandas()
    id_df.rename(columns={'month':'month_ts'}, inplace=True)
    id_df['xgb_pred'] = y_hat.astype('float32')
    pq.write_table(pa.Table.from_pandas(id_df),
                   f"{WORK_OUT}/xgb_{yr}.parquet", compression="zstd")
    print(f"✅ {yr}: {len(id_df):,} preds saved")

    pred_all.append(y_hat);  true_all.append(d_oos.get_label())
    del bst, d_train, d_oos, id_tbl, id_df; gc.collect()

# aggregate OOS performance
y_pred = np.concatenate(pred_all);  y_true = np.concatenate(true_all)
rmse   = np.sqrt(mean_squared_error(y_true, y_pred))
from sklearn.metrics import r2_score
print(f"OOS RMSE 1987-2016 (XGBoost) = {rmse:.5f}")
print(f"OOS R²                       = {r2_score(y_true, y_pred):.5f}")