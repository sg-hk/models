# -*- coding: utf-8 -*-
"""kelly_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_v8XcRFdDKcp8CSpNkzSWsKWE69TLKw
"""

# --- Core Imports ---
import os
import gc
import json
import functools
import numpy as np
import pandas as pd # For potential minor data inspection if needed

# --- Machine Learning Imports ---
# Scikit-learn for some models and utilities
from sklearn.linear_model import ElasticNet, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
# For GPU Random Forest and Gradient Boosted Trees (cuML)
try:
    import cudf
    import cupy as cp
    from cuml.ensemble import RandomForestRegressor as cuRF
    from cuml.ensemble import GradientBoostingRegressor as cuGBT # or xgboost if preferred
    # from cuml.model_selection import train_test_split # if needed
    print("cuML components imported successfully.")
    CUML_AVAILABLE = True
except ImportError:
    print("cuML not found. Will fall back to CPU versions if those are implemented later.")
    print("For GPU RF and GBT, ensure cuDF and cuML are installed in your environment.")
    CUML_AVAILABLE = False

# PyTorch for Neural Networks
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
print(f"PyTorch version: {torch.__version__}")

# --- Data Loading Utilities (from previous script) ---
import pyarrow.dataset as ds
from tqdm.auto import tqdm # For progress bars during any on-the-fly generation

# --- Google Drive Mount ---
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# --- Paths (Must match where your data was saved) ---
DATA_DIR = '/content/drive/MyDrive/capstone'
WORKDIR = os.path.join(DATA_DIR, 'tmp_tidy_rigorous_v3') # Points to the v3 processed data
BATCH_DIR = os.path.join(WORKDIR, "batches")

print(f"Modeling WORKDIR: {WORKDIR}")
print(f"Expecting pre-generated batches in: {BATCH_DIR}")

# --- Load Base Dataset Schema Information (for iter_batches_generator if needed) ---
BASE_PARQUET_PATH = os.path.join(WORKDIR, "base.parquet")
CHAR_COLS = []
MACRO_COLS = []
ID_LIKE = {"permno", "month", "month_int", "year"}

if os.path.exists(BASE_PARQUET_PATH):
    BASE_DS_SCHEMA = ds.dataset(BASE_PARQUET_PATH, format="parquet").schema
    CHAR_COLS = [c for c in BASE_DS_SCHEMA.names if c.startswith("char_")]
    MACRO_COLS = [c for c in BASE_DS_SCHEMA.names if c.startswith("macro_")]
    print(f"Schema loaded: {len(CHAR_COLS)} char_cols, {len(MACRO_COLS)} macro_cols.")
else:
    print(f"ERROR: {BASE_PARQUET_PATH} not found! On-the-fly batch generation will fail.")

# --- Batch Data Loading Functions (adapted from your loading script) ---
@functools.lru_cache(maxsize=len(MACRO_COLS) if MACRO_COLS else 8)
def _inter_ds_loader(macro_col_name_with_prefix):
    if not os.path.isdir(WORKDIR): return None
    macro_name_without_prefix = macro_col_name_with_prefix.replace("macro_", "")
    shard_path = os.path.join(WORKDIR, f"interact_{macro_name_without_prefix}.parquet")
    if not os.path.exists(shard_path): return None
    try: return ds.dataset(shard_path, format="parquet")
    except Exception: return None

def iter_batches_generator(year_slice=None, batch_rows=200_000):
    base_ds_for_iter = ds.dataset(BASE_PARQUET_PATH, format="parquet") # Load fresh for iterator
    if base_ds_for_iter is None: return

    filt = None
    if year_slice:
        lo, hi = year_slice
        filt = (ds.field("year") >= lo) & (ds.field("year") <= hi)

    row_ptr = 0
    for rb in base_ds_for_iter.to_batches(batch_size=batch_rows, filter=filt):
        if rb.num_rows == 0: continue
        pdf = rb.to_pandas()
        if "ret_excess" not in pdf.columns or "mktcap_lag" not in pdf.columns:
            row_ptr += rb.num_rows; gc.collect(); continue

        y = pdf.pop("ret_excess").to_numpy("float32", copy=False)
        size = pdf.pop("mktcap_lag").to_numpy("float32", copy=False).reshape(-1, 1)

        cols_to_drop_from_pdf = [col for col in ID_LIKE if col in pdf.columns]
        pdf.drop(columns=cols_to_drop_from_pdf, inplace=True)

        X_parts = [pdf.astype("float32", copy=False), size]

        for m_col_name_with_prefix in MACRO_COLS:
            interaction_dataset = _inter_ds_loader(m_col_name_with_prefix)
            num_chars = len(CHAR_COLS)
            fallback_zeros = np.zeros((rb.num_rows, num_chars), dtype="float32")

            if interaction_dataset is None:
                X_parts.append(fallback_zeros); continue

            macro_name_no_prefix = m_col_name_with_prefix.replace("macro_", "")
            interaction_col_names_in_shard = [f"{c}:{macro_name_no_prefix}" for c in CHAR_COLS]

            try:
                shard_schema_names = set(interaction_dataset.schema.names)
                if not all(col in shard_schema_names for col in interaction_col_names_in_shard):
                    X_parts.append(fallback_zeros); continue

                tbl = (interaction_dataset
                       .to_table(columns=interaction_col_names_in_shard)
                       .slice(row_ptr, rb.num_rows))
                if tbl.num_rows == 0 and rb.num_rows > 0 :
                    X_parts.append(fallback_zeros)
                else:
                    X_parts.append(tbl.to_pandas().astype("float32", copy=False))
            except Exception:
                X_parts.append(fallback_zeros)

        X_parts_valid = [part for part in X_parts if isinstance(part, (pd.DataFrame, np.ndarray)) and part.size > 0 and part.shape[0] == rb.num_rows]

        if len(X_parts_valid) != (2 + len(MACRO_COLS)): # Base_df_parts, size, and all interaction terms
            # This check ensures all parts are present and have the correct first dimension
            # print(f"Warning (iter_batches): Mismatch in X_parts_valid length or row counts at row_ptr {row_ptr}. Expected {2+len(MACRO_COLS)} parts, got {len(X_parts_valid)}.")
            row_ptr += rb.num_rows; del pdf, X_parts, X_parts_valid; gc.collect(); continue

        try:
            arrays_to_stack = [(a if isinstance(a, np.ndarray) else a.to_numpy("float32", copy=False)) for a in X_parts_valid]
            X = np.hstack(arrays_to_stack)
            yield X, y
        except ValueError as e:
            print(f"Error during hstack at {row_ptr} (slice {year_slice}): {e}")
            for i, arr_info in enumerate(arrays_to_stack): print(f"Shape of part {i}: {arr_info.shape}")

        row_ptr += rb.num_rows
        del pdf, X_parts, X_parts_valid, arrays_to_stack, X, y; gc.collect()

def collect_batches(year_lo, year_hi, force_regenerate=False):
    fname = os.path.join(BATCH_DIR, f"year_{year_lo}_{year_hi}.npz")

    if not force_regenerate and os.path.exists(fname):
        try:
            data = np.load(fname)
            if 'X' in data and 'y' in data:
                 print(f"✅ Successfully loaded pre-generated batch from: {fname}")
                 return data['X'], data['y']
            else: print(f"Cached file {fname} incomplete. Regenerating.")
        except Exception as e: print(f"Error loading {fname}: {e}. Regenerating.")
    else:
        if force_regenerate: print(f"Forcing regeneration for {year_lo}-{year_hi}.")
        else: print(f"Batch file not found: {fname}. Generating on-the-fly.")

    xs, ys = [], []
    for Xb, yb in tqdm(iter_batches_generator(year_slice=(year_lo, year_hi)), desc=f"Generating {year_lo}-{year_hi}"):
        if Xb is not None and yb is not None: xs.append(Xb); ys.append(yb)

    if not xs: return None, None

    try:
        X = np.vstack(xs); y = np.concatenate(ys)
        os.makedirs(os.path.dirname(fname), exist_ok=True)
        np.savez_compressed(fname, X=X, y=y)
        print(f"✅ Generated and saved batch: {fname}")
        return X, y
    except ValueError as e:
        print(f"Error vstack/concatenate for {year_lo}-{year_hi}: {e}"); return None, None

print("\n--- Setup for data loading in modeling workbook is complete ---")
print("Call `collect_batches(year_lo, year_hi)` to load data.")

# --- Example: Load the main data splits ---
print("\nLoading main data splits...")
X_train, y_train = collect_batches(1957, 1974)
X_valid, y_valid = collect_batches(1975, 1986)
X_test, y_test = collect_batches(1987, 2016) # This was generated on-the-fly in your previous output, so it should load from cache now

# --- Verify Shapes (optional) ---
if X_train is not None: print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
if X_valid is not None: print(f"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}")
if X_test is not None: print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

# --- Check if GPU is available for PyTorch and cuML ---
if CUML_AVAILABLE:
    print(f"\ncuML is available. GPU-accelerated Random Forest and GBT can be used.")
    num_gpus = cp.cuda.runtime.getDeviceCount()
    if num_gpus > 0:
        print(f"Found {num_gpus} CUDA GPU(s).")
        cp.cuda.runtime.setDevice(0) # Set default device to GPU 0
        print(f"Using GPU: {cp.cuda.runtime.getDeviceProperties(0)['name']}")
    else:
        print("cuML imported but no CUDA GPUs found by Cupy.")
else:
    print("\ncuML not available. Models requiring it will not run on GPU.")

if torch.cuda.is_available():
    print(f"PyTorch CUDA is available. Version: {torch.version.cuda}")
    device = torch.device("cuda")
    print(f"PyTorch using device: {torch.cuda.get_device_name(0)}")
else:
    print("PyTorch CUDA not available. Neural Networks will run on CPU.")
    device = torch.device("cpu")

# Imports specifically for this NN training script
# Assuming torch, nn, np, os, gc, pq, ds, mean_squared_error, r2_score, tqdm
# were in the previous block. If not, they'd be needed here.
# The user asked to remove imports already present, so I'll be minimal.
# We definitely need these for PyTorch model definition and training.
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np # Already imported, but good to be explicit for usage
import os # Already imported
import gc # Already imported
import pyarrow.parquet as pq # For reading predictions
import pyarrow.dataset as ds # For fetching IDs
from sklearn.metrics import mean_squared_error, r2_score # For final eval
from tqdm.auto import tqdm # For progress bars

# Assuming DEVICE, WORKDIR, BASE_DS, predictor_cols_loaded, collect_batches are defined
# in the preceding code block (your data loading block)

# --- Configuration for Neural Network ---
# DEVICE should be set in the previous block, e.g., device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# EPOCHS = 50 # Original value
# LR = 5e-4 # Original value
EPOCHS = 5 # REDUCED FOR QUICKER TEST RUNS - USER SHOULD ADJUST
LR = 1e-3  # ADJUSTED FOR POTENTIAL QUICKER CONVERGENCE TEST - USER SHOULD TUNE

OUT_DIR_NN = os.path.join(WORKDIR, "nn_oos_preds_final_v2") # Use a new versioned output
os.makedirs(OUT_DIR_NN, exist_ok=True)
print(f"NN OOS predictions will be saved to: {OUT_DIR_NN}")

# Determine feature dimensions from the loaded data
# X_temp_for_dim, _ = collect_batches(1957, 1957) # Load a small batch to get dimensions
# if X_temp_for_dim is None:
#     raise ValueError("Could not load a sample batch to determine feature dimensions.")
# D_IN_FEATURES = X_temp_for_dim.shape[1] # Total number of features from collect_batches
# print(f"Input dimension (D_IN_FEATURES) for MLP: {D_IN_FEATURES}")
# del X_temp_for_dim; gc.collect()
# Hardcoding D_IN_FEATURES based on previous successful load (1025) for now.
# It's better to get this dynamically if possible, but the user output confirmed 1025.
D_IN_FEATURES = 1025
if D_IN_FEATURES != 1025:
    print(f"WARNING: D_IN_FEATURES is {D_IN_FEATURES}, expected 1025. Check data loading.")


# --- Simple MLP Model Definition ---
class MLP(nn.Module):
    def __init__(self, d_in):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(d_in, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 1)
        )
    def forward(self, x):
        return self.network(x)

# --- Helper function to get raw data ---
def get_raw_data_for_year_span(year_lo, year_hi):
    X_cpu_raw, y_cpu_raw = collect_batches(year_lo, year_hi)
    if X_cpu_raw is None or y_cpu_raw is None:
        print(f"Warning: No data returned from collect_batches for {year_lo}-{year_hi}")
        return None, None, None

    # Filter out rows where y_cpu_raw is NaN or Inf (target variable issues)
    finite_y_mask = np.isfinite(y_cpu_raw)
    X_cpu_raw = X_cpu_raw[finite_y_mask]
    y_cpu_raw = y_cpu_raw[finite_y_mask]

    if X_cpu_raw.shape[0] == 0:
        print(f"Warning: No valid (finite y) data for {year_lo}-{year_hi} after filtering.")
        return None, None, None

    # The original mask 'm' in the user's script was based on this.
    # We return the mask relative to the original query before y-filtering,
    # but it's simpler to return the filtered data and the indices of these rows
    # if we need to map back to original ID_DF.
    # For now, the mask 'm' will be effectively the selection of finite_y_mask.
    # The 'm' in the original script was used to slice id_df.
    # We'll fetch id_df for the year and then use this new mask length.
    return X_cpu_raw, y_cpu_raw, finite_y_mask # Returning the mask might be complex if collect_batches already filters.
                                             # The `finite_y_mask` is against `y_cpu_raw` *before* it's potentially modified by `collect_batches` internal filtering.
                                             # For simplicity, we will filter IDs based on the shape of the final X_oos, y_oos.

def free_gpu_mem():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# --- Training Loop ---
oos_predictions_all_years = []
oos_true_values_all_years = []
oos_ids_all_years = [] # To store permno, month for verification

for current_oos_year in tqdm(range(1987, 2017), desc="NN OOS Annual Retraining"):
    # Path for saving/loading predictions for the current OOS year
    predictions_fpath = os.path.join(OUT_DIR_NN, f"nn_preds_{current_oos_year}.parquet")

    if os.path.exists(predictions_fpath):
        print(f"Loading existing predictions for OOS year {current_oos_year} from {predictions_fpath}")
        pred_df = pq.read_table(predictions_fpath).to_pandas()
        # Assuming saved parquet has 'permno', 'month', 'nn_pred', 'ret_excess_true'
        # We need to ensure alignment if we mix loaded and newly generated for final metrics
        # For simplicity in this loop, if file exists, we assume it's complete for the year.
        # The final metrics calculation will handle concatenation.
        # For this example, let's just append to main lists if needed for overall R2.
        # For now, the script aims to generate if not exists.
        # To properly use loaded data for R2:
        # _, y_true_cpu_oos_raw, _ = get_raw_data_for_year_span(current_oos_year, current_oos_year)
        # if y_true_cpu_oos_raw is not None and not pred_df.empty:
        #     # This requires careful alignment of pred_df with y_true_cpu_oos_raw
        #     # For now, if file exists, we skip to avoid re-running,
        #     # and the final R2 will be based on what's appended.
        #     # For robust R2, load all parquet files at the end.
        #     oos_predictions_all_years.append(pred_df['nn_pred'].values)
        #     oos_true_values_all_years.append(pred_df['ret_excess_true'].values) # Assuming it was saved
        print(f"↩︎ Skipping year {current_oos_year}, predictions already exist.")
        continue

    print(f"\n--- Processing OOS year: {current_oos_year} ---")

    # 1. Get Raw Training Data (expanding window)
    train_year_start = 1957
    train_year_end = current_oos_year - 1
    print(f"Fetching training data: {train_year_start}-{train_year_end}")
    X_tr_cpu_raw, y_tr_cpu_raw, _ = get_raw_data_for_year_span(train_year_start, train_year_end)

    if X_tr_cpu_raw is None or X_tr_cpu_raw.shape[0] == 0:
        print(f"No training data available for OOS year {current_oos_year}. Skipping.")
        continue

    # 2. Get Raw OOS Data (current year)
    print(f"Fetching OOS data: {current_oos_year}-{current_oos_year}")
    X_oos_cpu_raw, y_oos_cpu_raw, _ = get_raw_data_for_year_span(current_oos_year, current_oos_year)

    if X_oos_cpu_raw is None or X_oos_cpu_raw.shape[0] == 0:
        print(f"No OOS data available for year {current_oos_year}. Skipping.")
        continue

    # 3. Standardize Features
    # Calculate mean and std from the current training set
    mu = np.mean(X_tr_cpu_raw, axis=0)
    sigma = np.std(X_tr_cpu_raw, axis=0) + 1e-8 # Add epsilon for stability

    X_tr_cpu_std = (X_tr_cpu_raw - mu) / sigma
    X_oos_cpu_std = (X_oos_cpu_raw - mu) / sigma # Apply training set mu/sigma to OOS data

    # 4. Convert to PyTorch Tensors and move to DEVICE
    X_tr_tensor = torch.tensor(X_tr_cpu_std, dtype=torch.float32).to(device) # device from loading block
    y_tr_tensor = torch.tensor(y_tr_cpu_raw, dtype=torch.float32).unsqueeze(1).to(device) # Target needs to be [N,1] for MSELoss with [N,1] pred

    X_oos_tensor = torch.tensor(X_oos_cpu_std, dtype=torch.float32).to(device)
    # y_oos_tensor = torch.tensor(y_oos_cpu_raw, dtype=torch.float32).unsqueeze(1).to(device) # Not used in training loop for this year

    # 5. Initialize Model, Optimizer, Loss
    model = MLP(d_in=D_IN_FEATURES).to(device) # d_in uses all features
    optimizer = optim.AdamW(model.parameters(), lr=LR)
    loss_function = nn.MSELoss()

    # 6. Training Loop for the current OOS year
    print(f"Training model for OOS year {current_oos_year} with {X_tr_tensor.shape[0]} samples...")
    model.train()
    for epoch in range(EPOCHS):
        optimizer.zero_grad()
        predictions_tr = model(X_tr_tensor)
        loss = loss_function(predictions_tr, y_tr_tensor)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Original had 0.5, 1.0 is also common
        optimizer.step()
        # if (epoch + 1) % 10 == 0: # Optional: print epoch loss
        #     print(f"Year {current_oos_year}, Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.6f}")

    # 7. Get OOS Predictions
    model.eval()
    with torch.no_grad():
        y_hat_oos_tensor = model(X_oos_tensor)
        y_hat_oos_cpu = y_hat_oos_tensor.squeeze(1).cpu().numpy()

    # 8. Prepare data for saving (IDs + predictions + true values)
    # Fetch corresponding permno, month for the OOS predictions
    # The y_oos_cpu_raw and y_hat_oos_cpu are already filtered for finite initial y_oos_raw
    # and align with X_oos_cpu_raw.

    # We need to get the original permno/month for the rows in X_oos_cpu_raw / y_oos_cpu_raw
    # This requires loading the IDs for the year and then applying a mask that corresponds
    # to how y_oos_cpu_raw was filtered from the original y collected by collect_batches.
    # The `get_raw_data_for_year_span` filters y for finite values.
    # `y_oos_cpu_raw` are these filtered true values. `y_hat_oos_cpu` are the predictions for these.

    # Fetch all IDs for the OOS year first
    all_ids_oos_year_tbl = BASE_DS.to_table(
        columns=['permno', 'month'],
        filter=(ds.field("year") == current_oos_year)
    )
    all_ids_oos_year_df = all_ids_oos_year_tbl.to_pandas()

    # To align correctly, we need the original y values for the OOS year *before* filtering non-finites
    _, y_oos_original_for_ids, _ = collect_batches(current_oos_year, current_oos_year) # This y is already filtered by collect_batches
                                                                                     # if iter_batches itself filters, or if npz was saved pre-filtered
    # Let's assume collect_batches gives y that corresponds row-wise to X.
    # The finite_y_mask was created *inside* get_raw_data_for_year_span.
    # So, y_oos_cpu_raw corresponds to y_oos_original_for_ids[finite_y_mask_applied_inside_get_raw]
    # We need to reconstruct this.
    # Simpler: The number of rows in y_oos_cpu_raw is the number of valid predictions.
    # We need to find which original IDs these correspond to.

    # The `finite_y_mask` approach:
    # Step 1: Load raw X,y from collect_batches for the year
    X_oos_for_masking, y_oos_for_masking = collect_batches(current_oos_year, current_oos_year)
    if X_oos_for_masking is not None:
        finite_y_mask_for_year = np.isfinite(y_oos_for_masking)

        # Filter the full ID dataframe for the year using this mask
        # This assumes that collect_batches returns rows in the same order as a full table scan for that year
        if len(all_ids_oos_year_df) == len(finite_y_mask_for_year):
            ids_for_predictions_df = all_ids_oos_year_df[finite_y_mask_for_year].reset_index(drop=True)

            if len(ids_for_predictions_df) == len(y_hat_oos_cpu):
                results_df = ids_for_predictions_df.copy()
                results_df['nn_pred'] = y_hat_oos_cpu.astype('float32')
                results_df['ret_excess_true'] = y_oos_cpu_raw.astype('float32') # y_oos_cpu_raw IS the filtered true y

                pq.write_table(pa.Table.from_pandas(results_df), predictions_fpath, compression="zstd")
                print(f"✅ Year {current_oos_year}: {len(results_df):,} predictions saved to {predictions_fpath}")

                oos_predictions_all_years.append(y_hat_oos_cpu)
                oos_true_values_all_years.append(y_oos_cpu_raw)
            else:
                print(f"ERROR: Length mismatch for year {current_oos_year}. IDs: {len(ids_for_predictions_df)}, Preds: {len(y_hat_oos_cpu)}. Not saving.")
        else:
            print(f"ERROR: Length mismatch for year {current_oos_year}. All IDs: {len(all_ids_oos_year_df)}, Mask: {len(finite_y_mask_for_year)}. Not saving.")
    else:
        print(f"Could not retrieve data for year {current_oos_year} to create ID mask. Not saving.")

    # 9. Clean up
    del model, X_tr_tensor, y_tr_tensor, X_oos_tensor, optimizer, loss_function
    del X_tr_cpu_raw, y_tr_cpu_raw, X_oos_cpu_raw, y_oos_cpu_raw, X_tr_cpu_std, X_oos_cpu_std
    free_gpu_mem()

# --- Final Metrics Calculation (after all years) ---
if oos_predictions_all_years and oos_true_values_all_years:
    y_pred_final = np.concatenate(oos_predictions_all_years)
    y_true_final = np.concatenate(oos_true_values_all_years)

    # Final check for NaNs/Infs that might have slipped through or occurred during concatenation
    valid_mask_final = np.isfinite(y_true_final) & np.isfinite(y_pred_final)
    y_true_final_clean = y_true_final[valid_mask_final]
    y_pred_final_clean = y_pred_final[valid_mask_final]

    if len(y_true_final_clean) > 0:
        final_rmse = np.sqrt(mean_squared_error(y_true_final_clean, y_pred_final_clean))
        # The paper uses an R^2 benchmarked against a zero prediction for individual stocks.
        # R^2 = 1 - sum((y_true - y_pred)^2) / sum(y_true^2)
        # Ensure this is what you want for the final R2. Standard r2_score is 1 - SSR/SST
        #sst_benchmark_zero = np.sum(y_true_final_clean**2)
        #ssr = np.sum((y_true_final_clean - y_pred_final_clean)**2)
        #final_r2_vs_zero = 1 - (ssr / sst_benchmark_zero) if sst_benchmark_zero != 0 else -np.inf

        final_r2_sklearn = r2_score(y_true_final_clean, y_pred_final_clean) # Standard R2

        print(f"\n--- Final Aggregated OOS NN Performance (1987-2016) ---")
        print(f"Total valid predictions considered: {len(y_true_final_clean)}")
        print(f"RMSE:                             {final_rmse:.6f}")
        #print(f"R² (vs. Zero Prediction Benchmark): {final_r2_vs_zero:.6f}") # As per paper's metric style
        print(f"R² (sklearn default):             {final_r2_sklearn:.6f}")

    else:
        print("No valid predictions collected across all years to calculate final metrics.")
else:
    print("No OOS predictions were generated or collected. Final metrics cannot be calculated.")

print("\n--- Neural Network OOS Prediction Script Finished ---")