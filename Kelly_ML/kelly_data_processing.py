# -*- coding: utf-8 -*-
"""kelly_data-processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11D5Cr_RXdWjOP6CTZcVf16jmsPiThhHU

This processes CRSP, kelly, FRED, Goyal-Welch data for the models
"""

import numpy as np
import pandas as pd
import os
import gc
import json
import shutil
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds
from tqdm.auto import tqdm
import functools

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
DATA_DIR = '/content/drive/MyDrive/capstone'

TIDY_DATA_DIR_DRIVE = os.path.join(DATA_DIR, 'tmp_tidy_rigorous_v3')
os.makedirs(TIDY_DATA_DIR_DRIVE, exist_ok=True)

WORKDIR_LOCAL = "/content/tmp_tidy_local_rigorous_v3"
if os.path.exists(WORKDIR_LOCAL):
    shutil.rmtree(WORKDIR_LOCAL)
os.makedirs(WORKDIR_LOCAL, exist_ok=True)

FRED_CSV = os.path.join(DATA_DIR, 'fred.csv')
GOYAL_WELCH_CSV = os.path.join(DATA_DIR, 'goyal-welch_original.csv')
PNFI_CSV = os.path.join(DATA_DIR, 'PNFI.csv')
KN_CSV = os.path.join(DATA_DIR, 'K1NTOTL1ES000.csv')
KELLY_CSV = os.path.join(DATA_DIR, 'kelly.csv')
CRSP_CSV = os.path.join(DATA_DIR, 'crsp_2.csv')

print("--- Starting Step 1: Macroeconomic Predictors ---")

fred_tbl_raw = pd.read_csv(FRED_CSV, parse_dates=['observation_date'])
rf_monthly_df = (fred_tbl_raw
                .rename(columns={'observation_date': 'month', 'TB3MS': 'tbl_rate'})
                .assign(month=lambda d: d['month'].dt.to_period('M').dt.to_timestamp('M'),
                        tbl_rate=lambda d: d['tbl_rate'] / 100)
                .loc[:, ['month', 'tbl_rate']]
                .sort_values('month')
                .reset_index(drop=True))
print(f"FRED T-bill data loaded. Shape: {rf_monthly_df.shape}")

wg_raw = pd.read_csv(GOYAL_WELCH_CSV,
                     na_values=['NaN', ''],
                     converters={'Index': lambda s: str(s).replace(',', '')})
wg_raw['Index'] = pd.to_numeric(wg_raw['Index'], errors='coerce')

wg = (wg_raw
      .rename(columns={'yyyymm': 'month', 'b/m': 'bm'})
      .assign(month=lambda d: pd.to_datetime(d['month'], format='%Y%m').dt.to_period('M').dt.to_timestamp('M'))
     )

wg['D12_safe'] = wg['D12'].apply(lambda x: x if pd.notna(x) and x > 0 else np.nan)
wg['E12_safe'] = wg['E12'].apply(lambda x: x if pd.notna(x) and x > 0 else np.nan)
wg['Index_safe'] = wg['Index'].apply(lambda x: x if pd.notna(x) and x > 0 else np.nan)

wg['dp'] = np.log(wg['D12_safe']) - np.log(wg['Index_safe'])
wg['ep'] = np.log(wg['E12_safe']) - np.log(wg['Index_safe'])

wg = (wg
      .assign(tbl=lambda d: d['tbl'],
              lty=lambda d: d['lty'],
              dfy=lambda d: d['BAA'] - d['AAA'],
              tms=lambda d: d['lty'] - d['tbl'],
              dfr=lambda d: d['corpr'] - d['ltr'])
     )

wg_predictors_cols = ['dp', 'ep', 'bm', 'ntis', 'tbl', 'tms', 'dfy', 'svar']

wg_processed = (wg.loc[:, ['month'] + wg_predictors_cols]
                  .assign(**{col: lambda d, c=col: d[c].shift(1) for col in wg_predictors_cols})
                  .query('month >= "1957-01-01"')
                  .reset_index(drop=True))
print(f"Welch-Goyal data processed. Shape: {wg_processed.shape}")

pnfi = (pd.read_csv(PNFI_CSV, parse_dates=['DATE'])
        .rename(columns={'DATE': 'month', 'VALUE': 'pnfi'})
        .assign(month=lambda d: d['month'].dt.to_period('M').dt.to_timestamp('M'))
        .set_index('month')
        .resample('ME').ffill())

kn = (pd.read_csv(KN_CSV, parse_dates=['observation_date'])
      .rename(columns={'observation_date': 'month', 'K1NTOTL1ES000': 'kn'})
      .assign(month=lambda d: d['month'].dt.to_period('M').dt.to_timestamp('M'))
      .set_index('month')
      .resample('ME').ffill())

ik_df = (pd.concat([pnfi, kn], axis=1)
         .assign(ik=lambda d: d['pnfi'] / d['kn'])
         .loc[:, ['ik']]
         .assign(ik=lambda d: d['ik'].shift(1))
         .reset_index())
print(f"Investment-to-capital (ik) data processed. Shape: {ik_df.shape}")

macro_merged = pd.merge(wg_processed, ik_df, on='month', how='left')
final_macro_cols = wg_predictors_cols + ['ik']
macro_df = (macro_merged.loc[:, ['month'] + final_macro_cols]
            .add_prefix('macro_')
            .rename(columns={'macro_month': 'month'}))

for col in [f'macro_{c}' for c in final_macro_cols]:
    if col in macro_df.columns:
         macro_df[col] = macro_df[col].fillna(0.0)

macro_df[[c for c in macro_df.columns if c != 'month']] = \
    macro_df[[c for c in macro_df.columns if c != 'month']].astype('float32')
print(f"Final macro data prepared. Shape: {macro_df.shape}")

print("--- Starting Step 2: CRSP Stock Data ---")
crsp_raw = pd.read_csv(CRSP_CSV, parse_dates=['date'], low_memory=False)

crsp = (crsp_raw
        .rename(columns={'PERMNO': 'permno', 'date': 'month'})
        .assign(month=lambda d: pd.to_datetime(d['month']).dt.to_period('M').dt.to_timestamp('M'),
                PRC=lambda d: pd.to_numeric(d['PRC'], errors='coerce').abs(),
                SHROUT=lambda d: pd.to_numeric(d['SHROUT'], errors='coerce') * 1000,
                RET=lambda d: pd.to_numeric(d['RET'], errors='coerce').fillna(0.0),
                DLRET=lambda d: pd.to_numeric(d['DLRET'], errors='coerce').fillna(0.0))
        .loc[:, ['permno', 'month', 'PRC', 'SHROUT', 'RET', 'DLRET', 'SHRCD', 'EXCHCD', 'SICCD']]
        .assign(mktcap=lambda d: d['PRC'] * d['SHROUT'],
                totret=lambda d: (1 + d['RET']) * (1 + d['DLRET']) - 1)
        .sort_values(['permno', 'month'])
        .reset_index(drop=True))

crsp = pd.merge(crsp, rf_monthly_df.rename(columns={'tbl_rate':'rf_month'}), on='month', how='left')
crsp['exret'] = crsp['totret'] - crsp['rf_month']
crsp['rf_month'] = crsp['rf_month'].fillna(0.0)
crsp['exret'] = crsp['exret'].fillna(crsp['totret'] - crsp['rf_month'])


crsp['mktcap_lag'] = crsp.groupby('permno')['mktcap'].shift(1)

crsp_processed = crsp[['permno', 'month', 'exret', 'mktcap', 'mktcap_lag', 'SHRCD', 'EXCHCD', 'SICCD']].copy()
crsp_processed[['exret', 'mktcap', 'mktcap_lag']] = \
    crsp_processed[['exret', 'mktcap', 'mktcap_lag']].astype('float32')
print(f"CRSP data processed. Shape: {crsp_processed.shape}")

print("--- Starting Step 3: Kelly Firm Characteristics ---")
kelly_raw = pd.read_csv(KELLY_CSV, parse_dates=['DATE'], low_memory=False)
print(f"Original kelly_raw columns: {list(kelly_raw.columns)}")

kelly = (kelly_raw
         .rename(columns={'PERMNO': 'permno', 'DATE': 'month'})
         .assign(month=lambda d: d['month'].dt.to_period('M').dt.to_timestamp('M'))
         .sort_values(['permno', 'month'])
         .reset_index(drop=True))

original_kelly_cols_from_file = ['permno', 'DATE', 'mvel1', 'beta', 'betasq', 'chmom', 'dolvol', 'idiovol', 'indmom', 'mom1m', 'mom6m', 'mom12m', 'mom36m', 'pricedelay', 'turn', 'absacc', 'acc', 'age', 'agr', 'bm', 'bm_ia', 'cashdebt', 'cashpr', 'cfp', 'cfp_ia', 'chatoia', 'chcsho', 'chempia', 'chinv', 'chpmia', 'convind', 'currat', 'depr', 'divi', 'divo', 'dy', 'egr', 'ep', 'gma', 'grcapx', 'grltnoa', 'herf', 'hire', 'invest', 'lev', 'lgr', 'mve_ia', 'operprof', 'orgcap', 'pchcapx_ia', 'pchcurrat', 'pchdepr', 'pchgm_pchsale', 'pchquick', 'pchsale_pchinvt', 'pchsale_pchrect', 'pchsale_pchxsga', 'pchsaleinv', 'pctacc', 'ps', 'quick', 'rd', 'rd_mve', 'rd_sale', 'realestate', 'roic', 'salecash', 'saleinv', 'salerec', 'secured', 'securedind', 'sgr', 'sin', 'sp', 'tang', 'tb', 'aeavol', 'cash', 'chtx', 'cinvest', 'ear', 'nincr', 'roaq', 'roavol', 'roeq', 'rsup', 'stdacc', 'stdcf', 'ms', 'baspread', 'ill', 'maxret', 'retvol', 'std_dolvol', 'std_turn', 'zerotrade', 'sic2']
non_char_ids_in_kelly_original = ['permno', 'DATE', 'sic2']
potential_char_cols = [c for c in original_kelly_cols_from_file if c.lower() not in non_char_ids_in_kelly_original and c != 'month']
print(f"Identified potential_char_cols: {potential_char_cols}")

CHAR_UPDATE_FREQUENCY = {
    'mom1m': 'monthly', 'mom6m': 'monthly', 'mom12m': 'monthly', 'mom36m': 'monthly',
    'dolvol': 'monthly', 'turn': 'monthly', 'zerotrade': 'monthly',
    'mvel1': 'monthly',
    'beta': 'monthly', 'betasq': 'monthly', 'idiovol': 'monthly', 'retvol': 'monthly',
    'indmom': 'monthly', 'chmom': 'monthly',
    'pricedelay': 'monthly',
    'maxret': 'monthly',
    'std_dolvol': 'monthly', 'std_turn': 'monthly',
    'aeavol': 'monthly',
    'baspread': 'monthly', 'ill': 'monthly',
    'ep': 'quarterly', 'sp': 'quarterly',
    'absacc': 'quarterly', 'acc': 'quarterly', 'pctacc': 'quarterly', 'stdacc': 'quarterly',
    'age': 'annual',
    'agr': 'quarterly',
    'bm': 'annual', 'bm_ia': 'annual',
    'cashdebt': 'quarterly', 'cashpr': 'quarterly', 'cash': 'quarterly',
    'cfp': 'quarterly', 'cfp_ia': 'quarterly', 'stdcf': 'quarterly',
    'chatoia': 'quarterly', 'chcsho': 'quarterly', 'chempia': 'quarterly', 'chinv': 'quarterly', 'chpmia': 'quarterly',
    'convind': 'annual',
    'currat': 'quarterly',
    'depr': 'quarterly',
    'divi': 'annual', 'divo': 'annual', 'dy': 'monthly',
    'egr': 'quarterly',
    'gma': 'quarterly',
    'grcapx': 'quarterly', 'grltnoa': 'quarterly',
    'herf': 'annual',
    'hire': 'quarterly',
    'invest': 'quarterly',
    'lev': 'quarterly', 'lgr': 'quarterly',
    'mve_ia': 'annual',
    'operprof': 'quarterly', 'orgcap': 'annual',
    'pchcapx_ia': 'quarterly', 'pchcurrat': 'quarterly', 'pchdepr': 'quarterly', 'pchgm_pchsale': 'quarterly', 'pchquick': 'quarterly',
    'pchsale_pchinvt': 'quarterly', 'pchsale_pchrect': 'quarterly', 'pchsale_pchxsga': 'quarterly', 'pchsaleinv': 'quarterly',
    'ps': 'quarterly',
    'quick': 'quarterly',
    'rd': 'annual', 'rd_mve': 'annual', 'rd_sale': 'annual',
    'realestate': 'annual',
    'roic': 'quarterly', 'roaq': 'quarterly', 'roeq': 'quarterly',
    'rsup': 'quarterly', 'ear': 'quarterly',
    'salecash': 'quarterly', 'saleinv': 'quarterly', 'salerec': 'quarterly',
    'secured': 'annual', 'securedind': 'annual',
    'sgr': 'quarterly',
    'sin': 'annual',
    'tang': 'annual',
    'tb': 'annual',
    'chtx': 'quarterly',
    'cinvest': 'quarterly',
    'nincr': 'quarterly',
    'roavol': 'quarterly',
    'ms': 'quarterly'
}

actual_char_cols_to_process = []
for p_col in potential_char_cols:
    if p_col not in CHAR_UPDATE_FREQUENCY:
        print(f"WARNING: Characteristic '{p_col}' is missing in CHAR_UPDATE_FREQUENCY. Defaulting to 'annual'. YOU MUST VERIFY THIS.")
        CHAR_UPDATE_FREQUENCY[p_col] = 'annual'
    if p_col in kelly.columns : # Check if column actually exists in kelly dataframe
        actual_char_cols_to_process.append(p_col)
    else:
        print(f"INFO: Column '{p_col}' from header list not found in loaded kelly_raw DataFrame after initial processing. Skipping.")


print(f"Actual characteristics to process: {actual_char_cols_to_process}")
print(f"Number of actual characteristics: {len(actual_char_cols_to_process)}")

lag_map_fixed = {'monthly': 1, 'quarterly': 4, 'annual': 6}
print("Lagging characteristics...")
for col_name in tqdm(actual_char_cols_to_process, desc="Lagging characteristics"):
    freq = CHAR_UPDATE_FREQUENCY.get(col_name)
    if freq:
        lag_months = lag_map_fixed.get(freq)
        if lag_months is not None:
            kelly[col_name] = kelly.groupby('permno')[col_name].shift(lag_months)
        else:
            print(f"Warning: Unknown frequency '{freq}' for characteristic '{col_name}'. Not lagged.")

def rank_scale(series):
    return series.rank(pct=True, method='average', na_option='keep') * 2 - 1

print("Ranking and scaling characteristics...")
for col in tqdm(actual_char_cols_to_process, desc="Scaling characteristics"):
    if col in kelly.columns and pd.api.types.is_numeric_dtype(kelly[col]):
        kelly[col] = kelly.groupby('month')[col].transform(rank_scale)

print("Imputing missing characteristics (median)...")
for col in tqdm(actual_char_cols_to_process, desc="Median imputation"):
     if col in kelly.columns and pd.api.types.is_numeric_dtype(kelly[col]):
        median_val = kelly.groupby('month')[col].transform('median')
        kelly[col] = kelly[col].fillna(median_val)

print("Imputing remaining missing characteristics (zero)...")
for col in actual_char_cols_to_process:
    if col in kelly.columns and pd.api.types.is_numeric_dtype(kelly[col]):
        kelly[col] = kelly[col].fillna(0.0)

char_col_map = {col: f'char_{col}' for col in actual_char_cols_to_process}
kelly_processed = kelly.rename(columns=char_col_map)
final_char_cols = list(char_col_map.values())

for col in final_char_cols:
    if col in kelly_processed.columns:
        kelly_processed[col] = kelly_processed[col].astype('float32')

cols_to_select_from_kelly = ['permno', 'month'] + final_char_cols
if 'sic2' in kelly_processed.columns:
     cols_to_select_from_kelly.append('sic2')
elif 'SICCD' in kelly_processed.columns: # If original SICCD was kept through renames
    cols_to_select_from_kelly.append('SICCD')


kelly_to_merge = kelly_processed[cols_to_select_from_kelly].copy()
print(f"Kelly characteristics processed. Shape: {kelly_to_merge.shape}")

print("--- Starting Step 4: Merging All Datasets ---")
merged_df = pd.merge(kelly_to_merge,
                     crsp_processed,
                     on=['permno', 'month'],
                     how='inner')
print(f"Shape after Kelly-CRSP merge: {merged_df.shape}")

if 'sic2' not in merged_df.columns and 'SICCD_x' in merged_df.columns : # from CRSP
    merged_df['sic2'] = merged_df['SICCD_x'].astype(str).str[:2].fillna('UNKNOWN')
elif 'sic2' not in merged_df.columns and 'SICCD_y' in merged_df.columns: # from Kelly if it was named SICCD
     merged_df['sic2'] = merged_df['SICCD_y'].astype(str).str[:2].fillna('UNKNOWN')
elif 'sic2' not in merged_df.columns and 'SICCD' in merged_df.columns: # from kelly if it was SICCD and not renamed
    merged_df['sic2'] = merged_df['SICCD'].astype(str).str[:2].fillna('UNKNOWN')


if 'sic2' in merged_df.columns:
    merged_df['sic2'] = merged_df['sic2'].astype(str).fillna('UNKNOWN').astype('category')
else:
    print("Warning: 'sic2' could not be derived. Industry dummies will be missing.")
    merged_df['sic2'] = 'UNKNOWN'
    merged_df['sic2'] = merged_df['sic2'].astype('category')

cols_to_drop_after_sic2_creation = ['SICCD_x', 'SICCD_y', 'SICCD']
for c_drop in cols_to_drop_after_sic2_creation:
    if c_drop in merged_df.columns:
        merged_df.drop(columns=[c_drop], inplace=True)


full_df = pd.merge(merged_df, macro_df, on='month', how='left')
print(f"Shape after Macro merge: {full_df.shape}")

full_df.rename(columns={'exret': 'ret_excess'}, inplace=True)
full_df.sort_values(['month', 'permno'], inplace=True)
full_df.reset_index(drop=True, inplace=True)

print("--- Starting Step 5: Final Imputations and Cleanup ---")
full_df['mktcap_lag'] = full_df.groupby('permno')['mktcap_lag'].ffill().fillna(0.0).astype('float32')

for col in [f'macro_{c}' for c in final_macro_cols]:
    if col in full_df.columns:
        full_df[col] = full_df[col].fillna(0.0).astype('float32')

print(f"Missing 'ret_excess' (target): {full_df['ret_excess'].isna().sum()}")
full_df['ret_excess'] = full_df['ret_excess'].fillna(0.0) # Impute missing target with 0 for now
print(f"Missing 'ret_excess' (target) after fill: {full_df['ret_excess'].isna().sum()}")


full_df.drop(columns=['SHRCD', 'EXCHCD'], inplace=True, errors='ignore')

predictor_columns_for_check = final_char_cols + \
                              [col for col in full_df.columns if col.startswith('macro_')] + \
                              ['mktcap_lag']
na_sum_predictors = full_df[predictor_columns_for_check].isna().sum().sum()
if na_sum_predictors != 0:
    print(f"WARNING: Found {na_sum_predictors} NAs in predictor columns after final imputation!")
    print(full_df[predictor_columns_for_check].isna().sum()[full_df[predictor_columns_for_check].isna().sum() > 0])
    for p_col_fill in predictor_columns_for_check:
        if p_col_fill in full_df.columns and full_df[p_col_fill].isna().any():
            full_df[p_col_fill] = full_df[p_col_fill].fillna(0.0)
    print("Filled remaining predictor NAs with 0 as a fallback.")

print(f"✅ Final merged DF → {full_df.shape[0]:,} rows × {full_df.shape[1]} columns")
print(full_df.head(3))

print("--- Starting Step 6: Data Storage (Parquet Format) ---")

if 'sic2' in full_df.columns:
    print("One-hot encoding sic2...")
    sic_dummies = pd.get_dummies(full_df["sic2"], prefix="industry", dtype="float32")
    base_df_pre_concat = pd.concat([full_df.drop(columns=['sic2']), sic_dummies], axis=1)
else:
    base_df_pre_concat = full_df.copy()

base_cols = ['permno', 'month', 'ret_excess', 'mktcap_lag'] + \
            final_char_cols + \
            [col for col in base_df_pre_concat.columns if col.startswith('macro_')] + \
            [col for col in base_df_pre_concat.columns if col.startswith('industry_')]

base_df_cols_exist = [col for col in base_cols if col in base_df_pre_concat.columns]
base_df = base_df_pre_concat[base_df_cols_exist].copy()


base_df['month_int'] = base_df['month'].dt.year * 100 + base_df['month'].dt.month
base_df['year'] = base_df['month'].dt.year.astype('int16')
base_df['month_int'] = base_df['month_int'].astype('int32')

base_parquet_path_local = os.path.join(WORKDIR_LOCAL, "base.parquet")
if not os.path.exists(base_parquet_path_local):
    print(f"Writing base.parquet to {base_parquet_path_local}...")
    table_to_write = pa.Table.from_pandas(base_df, preserve_index=False)
    pq.write_table(table_to_write, base_parquet_path_local, compression="zstd")
    del table_to_write; gc.collect()
    print("✅ base.parquet written locally.")
else:
    print(f"↩︎ base.parquet already exists locally at {base_parquet_path_local} – skipped write.")

print("Preparing interaction shards...")
char_cols_for_interactions = [col for col in final_char_cols if col in base_df.columns] # Ensure they exist in base_df
macro_cols_for_interactions = [col for col in base_df.columns if col.startswith('macro_')]

if not char_cols_for_interactions:
    print("WARNING: No characteristic columns found for interactions. Skipping interaction shards.")
else:
    char_block_numpy = base_df[char_cols_for_interactions].to_numpy(dtype="float32", copy=False)
    def write_interaction_shard(macro_name: str, chunk_rows: int = 500_000):
        out_path_local = os.path.join(WORKDIR_LOCAL, f"interact_{macro_name.replace('macro_', '')}.parquet")
        if os.path.exists(out_path_local):
            return

        macro_vec_numpy = base_df[macro_name].to_numpy(dtype="float32", copy=False)
        interaction_block = char_block_numpy * macro_vec_numpy[:, None]
        col_names_interaction = [f"{char_col}:{macro_name.replace('macro_', '')}" for char_col in char_cols_for_interactions]
        writer = None
        for r0 in range(0, len(base_df), chunk_rows):
            r1 = min(r0 + chunk_rows, len(base_df))
            current_chunk_arrays = [pa.array(interaction_block[r0:r1, j]) for j in range(interaction_block.shape[1])]
            tbl_chunk = pa.Table.from_arrays(current_chunk_arrays, names=col_names_interaction)
            if writer is None:
                writer = pq.ParquetWriter(out_path_local, tbl_chunk.schema, compression="zstd")
            writer.write_table(tbl_chunk)
            del tbl_chunk, current_chunk_arrays; gc.collect()
        if writer:
            writer.close()
        del interaction_block, macro_vec_numpy; gc.collect()

    for m_col in tqdm(macro_cols_for_interactions, desc="Writing interaction shards"):
        write_interaction_shard(m_col)
    print("✅ Interaction shards written locally.")


print(f"--- Starting Step 7: Copying Processed Data to {TIDY_DATA_DIR_DRIVE} ---")
if WORKDIR_LOCAL != TIDY_DATA_DIR_DRIVE:
    if not os.path.exists(TIDY_DATA_DIR_DRIVE):
        os.makedirs(TIDY_DATA_DIR_DRIVE, exist_ok=True)

    if os.path.exists(base_parquet_path_local):
        shutil.copy2(base_parquet_path_local, os.path.join(TIDY_DATA_DIR_DRIVE, "base.parquet"))
        print(f"Copied base.parquet to {TIDY_DATA_DIR_DRIVE}")

    copied_shards_count = 0
    for m_col in macro_cols_for_interactions:
        shard_filename = f"interact_{m_col.replace('macro_', '')}.parquet"
        local_shard_path = os.path.join(WORKDIR_LOCAL, shard_filename)
        drive_shard_path = os.path.join(TIDY_DATA_DIR_DRIVE, shard_filename)
        if os.path.exists(local_shard_path):
            shutil.copy2(local_shard_path, drive_shard_path)
            copied_shards_count +=1
    if macro_cols_for_interactions and char_cols_for_interactions :
        print(f"Copied {copied_shards_count} interaction shards to {TIDY_DATA_DIR_DRIVE}")
    print(f"✅ All processed files copied to {TIDY_DATA_DIR_DRIVE}")
else:
    print(f"↩︎ Local work directory is the same as final tidy directory. No copy needed.")

if WORKDIR_LOCAL != TIDY_DATA_DIR_DRIVE and os.path.exists(WORKDIR_LOCAL):
    print(f"Local temporary directory {WORKDIR_LOCAL} can be manually removed if desired.")

print("--- Data Processing Script Finished ---")

# --- STEP 8: PRE-GENERATE AND SAVE MODEL INPUTS (Appended Block) ---
print("--- Starting Step 8: Pre-generating and Saving Model Inputs ---")

# 8.1. Save predictor_cols.json
# final_char_cols should be the list of 'char_' prefixed characteristic names
# that are present in the final base_df
if 'base_df' in globals() and 'final_char_cols' in globals():
    char_cols_in_base_df = [col for col in final_char_cols if col in base_df.columns and col.startswith('char_')]
    PREDICTORS_JSON_PATH = os.path.join(DATA_DIR, "predictor_cols.json")
    with open(PREDICTORS_JSON_PATH, "w") as f:
        json.dump(char_cols_in_base_df, f)
    print(f"✅ Saved predictor columns ({len(char_cols_in_base_df)} cols) to {PREDICTORS_JSON_PATH}")
else:
    print("WARNING: 'base_df' or 'final_char_cols' not found in global scope. Cannot save predictor_cols.json.")
    print("Ensure the main processing script defines these variables and they are accessible.")
    char_cols_in_base_df = [] # Define to prevent error later if this path is taken

# 8.2. Define Paths and Constants for Batch Generation
# These paths should point to where the data processing script JUST saved the files
BASE_PARQUET_FOR_BATCHING = os.path.join(TIDY_DATA_DIR_DRIVE, "base.parquet")
INTERACTIONS_DIR_FOR_BATCHING = TIDY_DATA_DIR_DRIVE # Interaction shards are in the root of TIDY_DATA_DIR_DRIVE
BATCH_DIR_SAVE_LOCATION = os.path.join(TIDY_DATA_DIR_DRIVE, "batches")
os.makedirs(BATCH_DIR_SAVE_LOCATION, exist_ok=True)
print(f"Batches will be saved to: {BATCH_DIR_SAVE_LOCATION}")

# 8.3. Setup for Batch Iteration (Load Datasets and Schemas)
if not os.path.exists(BASE_PARQUET_FOR_BATCHING):
    print(f"ERROR: {BASE_PARQUET_FOR_BATCHING} not found. Cannot proceed with batch generation.")
else:
    BASE_DS_FOR_BATCHING = ds.dataset(BASE_PARQUET_FOR_BATCHING, format="parquet")
    CHAR_COLS_FOR_BATCHING = [c for c in BASE_DS_FOR_BATCHING.schema.names if c.startswith("char_")]
    MACRO_COLS_FOR_BATCHING = [c for c in BASE_DS_FOR_BATCHING.schema.names if c.startswith("macro_")]
    ID_LIKE_FOR_BATCHING = {"permno", "month", "month_int", "year"}

    print(f"Batch generation setup: {len(CHAR_COLS_FOR_BATCHING)} char_cols, {len(MACRO_COLS_FOR_BATCHING)} macro_cols.")

    # 8.4. Define Batch Generation Functions (Adapted from your loading script)
    @functools.lru_cache(maxsize=len(MACRO_COLS_FOR_BATCHING) if MACRO_COLS_FOR_BATCHING else 8)
    def _inter_ds_for_processing(macro_col_name_with_prefix):
        macro_name_without_prefix = macro_col_name_with_prefix.replace("macro_", "")
        shard_path = os.path.join(INTERACTIONS_DIR_FOR_BATCHING, f"interact_{macro_name_without_prefix}.parquet")
        if not os.path.exists(shard_path):
            print(f"WARNING (processing time): Interaction shard not found: {shard_path}")
            return None
        return ds.dataset(shard_path, format="parquet")

    def iter_batches_for_processing(year_slice=None, batch_rows=200_000):
        if 'BASE_DS_FOR_BATCHING' not in globals():
            print("ERROR (processing time): BASE_DS_FOR_BATCHING is not defined.")
            return

        filt = None
        if year_slice:
            lo, hi = year_slice
            filt = (ds.field("year") >= lo) & (ds.field("year") <= hi)

        row_ptr = 0
        for rb in BASE_DS_FOR_BATCHING.to_batches(batch_size=batch_rows, filter=filt):
            pdf = rb.to_pandas()
            if "ret_excess" not in pdf.columns or "mktcap_lag" not in pdf.columns:
                print("ERROR (processing time): 'ret_excess' or 'mktcap_lag' missing in batch.")
                continue

            y = pdf.pop("ret_excess").to_numpy("float32", copy=False)
            size = pdf.pop("mktcap_lag").to_numpy("float32", copy=False).reshape(-1, 1)

            cols_to_drop_from_pdf = [col for col in ID_LIKE_FOR_BATCHING if col in pdf.columns]
            pdf.drop(columns=cols_to_drop_from_pdf, inplace=True)

            X_parts = [pdf.astype("float32", copy=False), size]

            for m_col_name_with_prefix in MACRO_COLS_FOR_BATCHING:
                interaction_dataset = _inter_ds_for_processing(m_col_name_with_prefix)
                if interaction_dataset is None:
                    print(f"Skipping interaction with {m_col_name_with_prefix} for this batch (shard not found).")
                    num_chars = len(CHAR_COLS_FOR_BATCHING) # Fallback: add zeros
                    zero_interaction_block = np.zeros((rb.num_rows, num_chars), dtype="float32")
                    X_parts.append(zero_interaction_block)
                    continue

                macro_name_no_prefix = m_col_name_with_prefix.replace("macro_", "")
                interaction_col_names_in_shard = [f"{c}:{macro_name_no_prefix}" for c in CHAR_COLS_FOR_BATCHING]

                try:
                    tbl = (interaction_dataset
                           .to_table(columns=interaction_col_names_in_shard)
                           .slice(row_ptr, rb.num_rows))
                    X_parts.append(tbl.to_pandas().astype("float32", copy=False))
                except pa.ArrowInvalid as e:
                    print(f"Error reading interaction columns for {m_col_name_with_prefix} (processing time): {e}")
                    num_chars = len(CHAR_COLS_FOR_BATCHING) # Fallback: add zeros
                    zero_interaction_block = np.zeros((rb.num_rows, num_chars), dtype="float32")
                    X_parts.append(zero_interaction_block)

            X_parts_valid = [part for part in X_parts if part is not None and part.size > 0]
            if not X_parts_valid:
                print(f"Warning (processing time): No valid X_parts for batch starting at row_ptr {row_ptr}.")
                row_ptr += rb.num_rows; gc.collect(); continue

            try:
                arrays = [(a if isinstance(a, np.ndarray) else a.to_numpy("float32", copy=False)) for a in X_parts_valid]
                X = np.hstack(arrays)
                yield X, y
            except ValueError as e:
                print(f"Error during hstack for batch at row_ptr {row_ptr} (processing time): {e}")

            row_ptr += rb.num_rows
            del pdf, X_parts, X_parts_valid, arrays, X, y; gc.collect()

    def collect_batches_and_save(year_lo, year_hi, description=""):
        fname = os.path.join(BATCH_DIR_SAVE_LOCATION, f"year_{year_lo}_{year_hi}.npz")
        if os.path.exists(fname):
            print(f"Batch file {fname} already exists. Skipping generation.")
            return

        xs, ys = [], []
        for Xb, yb in tqdm(iter_batches_for_processing(year_slice=(year_lo, year_hi)), desc=f"Generating {description} ({year_lo}-{year_hi})"):
            if Xb is not None and yb is not None:
                 xs.append(Xb); ys.append(yb)

        if not xs:
            print(f"No data collected for year slice {year_lo}-{year_hi} ({description}). Batch file not saved.")
            return

        try:
            X = np.vstack(xs); y = np.concatenate(ys)
            np.savez_compressed(fname, X=X, y=y)
            print(f"✅ Saved pre-generated batch: {fname}")
        except ValueError as e:
            print(f"Error stacking/saving for {year_lo}-{year_hi} ({description}): {e}")

    # 8.5. Define Year Slices and Generate Batches
    # (Based on Gu, Kelly, Xiu (2020) sample splits)
    year_slices_to_generate = {
        "Train_Main": (1957, 1974),
        "Valid_Main": (1975, 1986),
        "Test_Main": (1987, 2016),
        # You can add more specific slices if your modeling requires them, e.g., for specific decades or rolling windows.
        # For a full out-of-sample period often used in papers:
        # "Full_OutOfSample_1975_2016": (1975, 2016), # Combining validation and test
        # "Full_Sample_1957_2016": (1957, 2016) # Entire dataset
    }

    # Determine the actual min and max year from the data to define a "Full_Sample" more dynamically
    if 'base_df' in globals() and not base_df.empty:
        min_data_year = base_df['year'].min()
        max_data_year = base_df['year'].max()
        year_slices_to_generate["Full_Sample_Dynamic"] = (min_data_year, max_data_year)
        print(f"Dynamically determined full sample range: {min_data_year}-{max_data_year}")


    print("\n--- Pre-generating specified batch files ---")
    for description, (year_lo, year_hi) in year_slices_to_generate.items():
        print(f"\nGenerating batch for: {description} ({year_lo}-{year_hi})")
        collect_batches_and_save(year_lo, year_hi, description=description)

print("\n--- Step 8: Pre-generation of Model Inputs Finished ---")